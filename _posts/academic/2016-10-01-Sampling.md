---
layout: post-no-feature
title: Sampling Methods on the YFCC 100M Dataset
description: "Exploiting unstructured photos from open source media (FlickR, Youtube, etc.) to build a model for image tagging and retrieval"
categories: articles
comments: true
date: 2016-10-01
tags: [software]
---


The following is work done at Lab41, though initially started at Lawrence Livermore National Laboratory.

By now you've heard of the [YFCC 100M Dataset](https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&did=67), and you may even have played around with the dataset. You'll start to notice that the metadata tags aren't very descriptive of the images themselves for most of the images. For example, take the below.

![Rosie](https://github.com/UCKarl/UCKarl.github.io/blob/master/_posts/20160327_141319.jpg?raw=true)


### Word2Vec

One of the more impactful papers in the past few years is Tomas Mikolov's word2vec.

#### The Trouble with GPUs

Mikolov's work is all done on multi-threaded CPUs, with good reason. The motherboard simply has way more memory. Secondly, he deals with only wide neural networks, which means that optimizing a single layer in parallel may be just fine. It's analogous to HOG-Wild, where you're just randomly optimizing columns.

She was sleeping one day (much like she was in the above picture), and I took a few pictures of her over several different angles, and then ran SfM algorithms to make a 3D rendering:

<html>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/FiNwu97TQ6w" frameborder="0" allowfullscreen></iframe>
</center>
</html>

<html>
<p>
<html>

The advantage of these methods is the fact that they can be used with images in the wild (for example, scrape FlickR, YouTube, or whatever), and organize them by using feature extractors and matching those features with each other. So, if run around . Below is a reconstruction that we built from 2317 images just running around the Stata Center near MIT campus. 

<html>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/CJkRyo-u1_U" frameborder="0" allowfullscreen></iframe>
</center>
</html>


Once you've reconstructed the city with images, you can align that with other interesting 3D point clouds from other modalities. For example, you can take a LiDAR point cloud, align it with the picture point cloud, and voila! You get this:


<html>
<center>
<iframe width="420" height="315" src="https://www.youtube.com/embed/oc1rTzwXbc8" frameborder="0" allowfullscreen></iframe>
</center>
</html>


If you've done everything right, you should be able take any new photo, and then align that new photo with the point cloud to figure out where you are!


<html>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/wPtGyScg8SQ" frameborder="0" allowfullscreen></iframe>
</center>
</html>

