---
layout: post-no-feature
title: Sampling Methods on the YFCC 100M Dataset
description: "Image tagging and retrieval on large scale unstructured data using sampling methods."
categories: articles
comments: true
date: 2016-10-01
tags: [software]
---


The following is work done at Lab41, though initially started at Lawrence Livermore National Laboratory.

By now, maybe you've heard of the [YFCC 100M Dataset](https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&did=67), and you may even have played around with the dataset. The underlying reasoning for the initial release of the dataset is its noisiness. Still, there's nothing like actually looking at the data. You'll start to notice that the metadata tags sometimes very and sometimes not at all descriptive of the images themselves. For example, take the below.

![FlickR Creative Commons from https://www.flickr.com/photos/guenterleitenbauer/17272171332/in/photolist-UbULQ-oJxdAu-4ZhXEf-9UN1Bi-3m4YQy-sjhoMs-73J38d-j1dBZc-5Jv22Z-dYwK6H-Ldqa7-6gc4CL-7cXAsP-5B37Sm-FB9Hp-JbvZxC-pHgXDN-oCUpJ9-7RLduZ-eAW16B-poUzEv-6Q2wNH-bSyyp8-atdXmp-JjGiS4-oTGsxq-515ypv-HUwTYF-8YFaai-gmHyuJ-88pmb5-bECPMC-w4urw-JnPeit-HyNqxQ-hMrztt-f4hgYU-fdvTWF-4TMi9v-HvpJxY-gv1asQ-pRpjEW-HT8SbU-qjLVSo-bqDQCz-8vzYc4-bRrtNT-s6gE7k-7LtUp1-dfDc7T](https://raw.githubusercontent.com/UCKarl/UCKarl.github.io/master/_posts/academic/images/venice.jpg)
```
Metadata Tags: 2015, April, Austria, Canon, Guenter, Günter, Landscape, Leitenbauer, Wels, bild, bilder, canal, canale, city, flickr, foto, fotos, image, images, kanal, key, landschaft, photo, photos, picture, pictures, stadt, town, venedig, venetia, venezia, venice, wasser, water, www.leitenbauer.net, Österreich, burano, island, insel, isola
```

Yes, we'll see `canal, venice, water, architecture`, which perfectly represent the picture. But the other stuff is enough to throw you off. I'm guessing that this was an Austrian Tourist who has a Canon camera, and he went to Venice in April 2015. To get a machine to recognize that is difficult, but that's another story altogether. The YFCC chalenge has been to be able to decipher *content* from an image, and in that sense, the tags `2015, April, Austria, Canon, flickr, foto, fotos, isola` are noise.

That's actually a pretty good example of the tags. There are some that are just inexcusably bad.


### Word2Vec

One of the more impactful papers in the past few years is Tomas Mikolov's word2vec. The major takeaway from `word2vec` is their negative sampling approach, through a broader idea of noise contrastive estimation. There are several articles on the description of their cost function on ArXiv and a [previous blog post](https://gab41.lab41.org/anything2vec-e99ec0dc186#.ddnjxweeq), but the idea is that it approximates the binary cross-entropy function through sampling. This is especially attractive in the proposed approach (on ArXiv here) and submitted to CVPR, because user generated content like the YFCC image and text dataset have a very large label set (in the number of unique metadata tags).

#### The Trouble with GPUs

Mikolov's work is all done on multi-threaded CPUs, with good reason. The motherboard simply has way more memory. Secondly, he deals with only wide neural networks, which means that optimizing a single layer in parallel may be just fine. It's analogous to HOG-Wild, where you're just randomly optimizing columns.

#### Sampling Distributions

The beauty of it is, because we are sampling from a distribution, the words `panasonic` or `iphoneography`, which apply to a random assortment of images, will only serve to push images away from them during negative sampling. Meanwhile, posive sampling of images, where we sample according to a scaled inverse of the distribution, will pull images toward them of less frequent words.

Recall the word2vec cost function.

$$ \max_{v_i, v_o} \log \sigma( v_o^T v_i ) + \sum_n \mathbb{E}_{n\sim p(i)}\left[ \sigma (- v_n^T v_i) \right] $$

There is that pesky issue of scale. If you've got 600k unique words, your output matrix will be of size 600k, and if you've got a second to last layer at 4096 dimensions, then the dimensionality of that matrix will be $600k \times 4096$, a pretty large matrix to backpropagate.
